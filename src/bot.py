# src/bot.py
import os
import re
import json
import logging
import sqlite3
import asyncio
from pathlib import Path

import numpy as np
from numpy.linalg import norm

from aiogram import Bot, Dispatcher, F
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties
from aiogram.types import Message
from aiogram.filters import CommandStart

from openai import OpenAI

# -----------------------------
# Configuration / Env
# -----------------------------
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not TELEGRAM_TOKEN:
    raise RuntimeError("TELEGRAM_TOKEN not set in environment")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not set in environment")

# Paths
BASE_DIR = Path(__file__).resolve().parent
LAW_BASE_PATH = BASE_DIR / "law_base.json"    # ensure this file exists (created by parse_law.py)
EMBED_PATH = BASE_DIR / "embeddings" / "tk_vectors.pkl"

# -----------------------------
# Init clients
# -----------------------------
bot = Bot(token=TELEGRAM_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
dp = Dispatcher()
client = OpenAI(api_key=OPENAI_API_KEY)

# -----------------------------
# Load law base and embeddings
# -----------------------------
if not LAW_BASE_PATH.exists():
    logging.warning(f"law_base.json not found at {LAW_BASE_PATH}; bot will run but article search unavailable.")
    LEX_BASE = {}
else:
    with open(LAW_BASE_PATH, "r", encoding="utf-8") as f:
        LEX_BASE = json.load(f)

VECTORS = {}
if EMBED_PATH.exists():
    try:
        import pickle
        with open(EMBED_PATH, "rb") as f:
            VECTORS = pickle.load(f)
    except Exception as e:
        logging.exception("Failed to load embeddings: %s", e)
        VECTORS = {}
else:
    VECTORS = {}

# -----------------------------
# Constants
# -----------------------------
LEX_LINK = "https://lex.uz/docs/6257291"
UZ_KEYWORDS = [
    "modda", "mehnat", "ishchi", "ish", "mehnat kodeksi", "qonun", "ishdan", "ish joyi",
    "ta'til", "maosh", "oylik", "kompensats", "bekor", "ish vaqti", "shartnoma", "salom"
]
RU_KEYWORDS = [
    "—Å—Ç–∞—Ç—å—è", "—Ç—Ä—É–¥", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "—É–≤–æ–ª—å–Ω", "–æ—Ç–ø—É—Å–∫", "–±–æ–ª—å–Ω–∏—á", "–¥–æ–≥–æ–≤–æ—Ä", "–æ–∫–ª–∞–¥",
    "–∫–æ–º–ø–µ–Ω—Å–∞—Ü", "—Å–æ–∫—Ä–∞—â", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä", "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏", "–ø—Ä–∏—ë–º", "—Ä–∞–±–æ—Ç–∞"
]

# -----------------------------
# Utilities
# -----------------------------
def cosine(a, b):
    a = np.array(a); b = np.array(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return -1.0
    return float(np.dot(a, b) / denom)

def detect_language_by_text(text: str):
    """
    Heuristic language detection:
    - if text contains Cyrillic letters -> 'ru'
    - if text contains Latin letters and Uzbek keywords -> 'uz'
    - otherwise -> None (unknown)
    """
    s = text.strip()
    if not s:
        return None
    # Cyrillic detection
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", s):
        return "ru"
    # Uzbek keywords (latin)
    low = s.lower()
    for kw in UZ_KEYWORDS:
        if kw in low:
            return "uz"
    # quick check: if text contains any Latin letters and some uz tokens
    if re.search(r"[A-Za-z]", s):
        if any(k in low for k in ["ish", "modda", "mehnat", "qonun", "salom"]):
            return "uz"
    return None

# -----------------------------
# Simple state (in-memory)
# -----------------------------
# stores user -> language ("ru"/"uz")
USER_LANG = {}

# -----------------------------
# Commands
# -----------------------------
@dp.message(CommandStart())
async def cmd_start(message: Message):
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! üëã Send me a question about labour/HR or just write 'salom' to identify your language.\n\n"
        "Salom! üëã Mehnat/HR borasidagi savolingizni yozing yoki tilni aniqlash uchun 'salom' deb yozing."
    )

@dp.message(F.text & F.regex(r"^(ru|uz)\b", flags=re.IGNORECASE))
async def set_lang_command(message: Message):
    code = message.text.strip().lower().split()[0]
    if code in ("ru", "uz"):
        USER_LANG[message.from_user.id] = code
        if code == "ru":
            await message.answer("–Ø–∑—ã–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: –†—É—Å—Å–∫–∏–π üá∑üá∫")
        else:
            await message.answer("Til saqlandi: O ªzbekcha (latin) üá∫üáø")
    else:
        await message.answer("Send 'ru' or 'uz' to choose language.")

# -----------------------------
# Classifier: is this HR question?
# -----------------------------
async def classify_hr(question: str) -> str:
    try:
        prompt = (
            "–ö—Ä–∞—Ç–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Ç–µ–∫—Å—Ç: HR –∏–ª–∏ NOT_HR. HR ‚Äî –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É/—Ç—Ä—É–¥–æ–≤—ã–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º, "
            "–Ω–∞–ø—Ä–∏–º–µ—Ä: —É–≤–æ–ª—å–Ω–µ–Ω–∏–µ, –æ—Ç–ø—É—Å–∫, –±–æ–ª—å–Ω–∏—á–Ω—ã–π, –æ–ø–ª–∞—Ç–∞, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞, —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ, —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä –∏ —Ç.–ø. "
            "NOT_HR ‚Äî –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, —ç–º–æ—Ü–∏–∏, –ª–∏—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã.\n\n"
            f"–¢–µ–∫—Å—Ç: {question}\n\n–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ 'HR' –∏–ª–∏ 'NOT_HR'."
        )
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã ‚Äî –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ 'HR' –∏–ª–∏ 'NOT_HR'."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=6
        )
        out = resp.choices[0].message.content.strip().upper()
        if out.startswith("HR"):
            return "HR"
        return "NOT_HR"
    except Exception:
        low = question.lower()
        if any(k in low for k in ["—É–≤–æ–ª", "–æ—Ç–ø—É—Å–∫", "–±–æ–ª—å–Ω–∏—á", "–¥–æ–≥–æ–≤–æ—Ä", "—Ç—Ä—É–¥", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "modda", "ish", "mehnat"]):
            return "HR"
        return "NOT_HR"

# -----------------------------
# Helpers: extract explicit article number
# -----------------------------
def extract_explicit_article(text: str):
    low = text.lower()
    m1 = re.search(r"—Å—Ç–∞—Ç(?:—å—è|–∏)?\s*(\d{1,4})", low)
    if m1:
        return m1.group(1)
    m2 = re.search(r"(\d{1,4})\s*-\s*modda", low) or re.search(r"(\d{1,4})\s+modda\b", low)
    if m2:
        return m2.group(1)
    m3 = re.search(r"modda\s*(\d{1,4})", low)
    if m3:
        return m3.group(1)
    return None

# -----------------------------
# Core message handler
# -----------------------------
@dp.message(F.text)
async def handle_message(message: Message):
    uid = message.from_user.id
    text = (message.text or "").strip()
    if not text:
        await message.answer("–ù–µ –ø–æ–Ω—è–ª —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî –Ω–∞–ø–∏—à–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–æ–ø—Ä–æ—Å –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –∏–ª–∏ 'ru'/'uz' —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å —è–∑—ã–∫.")
        return

    lang = USER_LANG.get(uid)
    if not lang:
        lang = detect_language_by_text(text)

    if not lang:
        await message.answer(
            "–ù–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ 'ru' (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ) –∏–ª–∏ 'uz' (–¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ, lotin) ‚Äî –∑–∞—Ç–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å.\n\n"
            "Tilni aniqlay olmadim. Iltimos, 'ru' yoki 'uz' deb yozing va so'ng savolingizni yuboring."
        )
        return

    USER_LANG[uid] = lang
    await message.chat.do("typing")

    explicit = extract_explicit_article(text)
    if explicit:
        article_id = explicit
    else:
        article_id = None

    classification = "HR"
    try:
        classification = await classify_hr(text) if not explicit else "HR"
    except Exception:
        classification = "HR"

    if classification == "NOT_HR":
        if lang == "ru":
            reply = ("–ü—Ä–∏–≤–µ—Ç! üëã –£ –º–µ–Ω—è –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ ‚Äî —è HR-–±–æ—Ç –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–∞. "
                     "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ —É–≤–æ–ª—å–Ω–µ–Ω–∏—é, –æ—Ç–ø—É—Å–∫—É, –±–æ–ª—å–Ω–∏—á–Ω–æ–º—É, —Ç—Ä—É–¥–æ–≤—ã–º –¥–æ–≥–æ–≤–æ—Ä–∞–º –∏–ª–∏ –ø—Ä–∞–≤–∞–º —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ ‚Äî —Å–ø—Ä–∞—à–∏–≤–∞–π—Ç–µ.")
        else:
            reply = ("Salom! üëã Hammasi joyida ‚Äî men O ªzbekiston Mehnat Kodeksi bo‚Äòyicha HR-botman. "
                     "Agar ishdan bo ªshatish, ta ºtil, kasallik, mehnat shartnomalari yoki xodim huquqlari haqida savolingiz bo‚Äòlsa ‚Äî so‚Äòrang.")
        await message.answer(reply + f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {LEX_LINK}")
        return

    article_text = ""
    found_article = None

    if article_id:
        if str(article_id) in LEX_BASE:
            found_article = str(article_id)
            article_text = LEX_BASE[found_article].get(lang, "")
        else:
            found_article = None
            article_text = ""

    if not found_article and VECTORS:
        try:
            emb = client.embeddings.create(model="text-embedding-3-small", input=text)
            qvec = np.array(emb.data[0].embedding)
            best_id = None
            best_score = -999
            for aid, vec in VECTORS.items():
                sc = cosine(qvec, np.array(vec))
                if sc > best_score:
                    best_score = sc
                    best_id = aid
            if best_score is not None and best_score >= 0.23:
                found_article = best_id
                article_text = LEX_BASE.get(str(found_article), {}).get(lang, "") or LEX_BASE.get(str(found_article), {}).get("ru", "")
            else:
                found_article = None
                article_text = ""
        except Exception as e:
            logging.exception("Embeddings search failed: %s", e)
            found_article = None
            article_text = ""

    system_msg = (
        "–¢—ã ‚Äî HR-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –†–µ—Å–ø—É–±–ª–∏–∫–∏ –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω. "
        "–ï—Å–ª–∏ —Ç–µ–±–µ –ø–µ—Ä–µ–¥–∞–ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ ‚Äî –æ—Ç–≤–µ—á–∞–π –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–µ –Ω–∞–∑—ã–≤–∞–π –¥—Ä—É–≥–∏–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π. "
        "–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏: '–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ; —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ lex.uz' –∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä. "
        "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —è—Å–Ω–æ –∏ –¥–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏ –¥–ª—è –∫–∞–¥—Ä–æ–≤–∏–∫–∞."
    )

    if found_article and article_text:
        user_msg = (
            f"–í–æ–ø—Ä–æ—Å ({lang}): {text}\n\n"
            f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è #{found_article} (–ª–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞). –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n\n"
            f"–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:\n{article_text}\n\n"
            "–î–∞–π –∫—Ä–∞—Ç–∫–æ–µ –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏ –¥–ª—è –∫–∞–¥—Ä–æ–≤–∏–∫–∞."
        )
    else:
        user_msg = (
            f"–í–æ–ø—Ä–æ—Å ({lang}): {text}\n\n"
            "–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π. –î–∞–π –æ–±—â–∏–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç –∏ –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–¥–∞–∫—Ü–∏—é –Ω–∞ lex.uz."
        )

    try:
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.15,
            max_tokens=700
        )
        gpt_answer = completion.choices[0].message.content.strip()
    except Exception as e:
        logging.exception("GPT completion error: %s", e)
        if found_article and article_text:
            gpt_answer = "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ù–∏–∂–µ ‚Äî –Ω–∞–π–¥–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è:\n\n" + article_text
        else:
            gpt_answer = "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    footer = f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è: {LEX_LINK}"
    if found_article:
        header = f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {found_article}\n\n"
        await message.answer(header + gpt_answer + footer)
    else:
        await message.answer(gpt_answer + footer)

    try:
        conn = sqlite3.connect(str(BASE_DIR / "bot_logs.db"))
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                username TEXT,
                lang TEXT,
                question TEXT,
                answer TEXT,
                article INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cur.execute("""
            INSERT INTO logs (user_id, username, lang, question, answer, article)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            uid,
            message.from_user.username or "",
            lang,
            text,
            (("–ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: " + str(found_article) + "\n\n") if found_article else "") + gpt_answer,
            int(found_article) if found_article else None
        ))
        conn.commit()
        conn.close()
    except Exception as e:
        logging.exception("Failed to write log: %s", e)

async def main():
    logging.basicConfig(level=logging.INFO)
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
