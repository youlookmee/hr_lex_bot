# src/bot.py
"""
HR Law Bot (–≤–∞—Ä–∏–∞–Ω—Ç: –∂–∏–≤–æ–π —Å—Ç–∏–ª—å A ‚Äî –∫–∞–∫ ChatGPT)
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
 - src/law_base.json  (–≤ —Ñ–æ—Ä–º–∞—Ç–µ {"157": {"ru": "...", "uz": "..."}, ...})
 - src/embeddings/tk_vectors.pkl  (pickle: dict article_id -> vector)
 - OPENAI_API_KEY –∏ TELEGRAM_TOKEN –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏
 - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: aiogram, openai, numpy, python-dotenv (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
"""

import os
import re
import json
import logging
import asyncio
from pathlib import Path

import numpy as np
from numpy.linalg import norm

from aiogram import Bot, Dispatcher, F
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties
from aiogram.types import Message
from aiogram.filters import CommandStart

from openai import OpenAI

# -----------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# -----------------------
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not TELEGRAM_TOKEN:
    raise RuntimeError("TELEGRAM_TOKEN not set in environment")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not set in environment")

# –ü—É—Ç–∏ (—Ä–∞–±–æ—Ç–∞–µ–º –≤–Ω—É—Ç—Ä–∏ /app/ –∏–ª–∏ –∫–æ—Ä–Ω—è —Ä–µ–ø–æ)
BASE_DIR = Path(__file__).resolve().parent
LAW_PATH = BASE_DIR / "law_base.json"
EMBED_PATH = BASE_DIR / "embeddings" / "tk_vectors.pkl"

LEX_LINK = "https://lex.uz/docs/6257291"

# -----------------------
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
# -----------------------
bot = Bot(token=TELEGRAM_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
dp = Dispatcher()
client = OpenAI(api_key=OPENAI_API_KEY)

# -----------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ law_base.json
# -----------------------
if LAW_PATH.exists():
    with open(LAW_PATH, "r", encoding="utf-8") as f:
        try:
            LEX_BASE = json.load(f)
        except Exception as e:
            logging.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å law_base.json: %s", e)
            LEX_BASE = {}
else:
    logging.warning("law_base.json not found at %s", LAW_PATH)
    LEX_BASE = {}

# -----------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ embeddings (–µ—Å–ª–∏ –µ—Å—Ç—å)
# -----------------------
VECTORS = {}
if EMBED_PATH.exists():
    try:
        import pickle
        with open(EMBED_PATH, "rb") as f:
            VECTORS = pickle.load(f)
    except Exception:
        logging.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å embeddings; semantic search –æ—Ç–∫–ª—é—á—ë–Ω.")
        VECTORS = {}
else:
    logging.info("Embeddings not found: %s ‚Äî semantic search disabled.", EMBED_PATH)

# -----------------------
# –£—Ç–∏–ª–∏—Ç—ã
# -----------------------
def cosine(a, b):
    a = np.array(a); b = np.array(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return -1.0
    return float(np.dot(a, b) / denom)

# –ø—Ä–æ—Å—Ç–æ–π –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ (–ª–∞—Ç–∏–Ω–∏—Ü–∞)
UZ_KEYWORDS = ["modda", "mehnat", "ishchi", "ish", "mehnat kodeksi", "qonun", "ta'til", "maosh", "ishlash", "bekor"]
# —Ä—É—Å—Å–∫–∏–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
RU_KEYWORDS = ["—Å—Ç–∞—Ç—å—è", "—Ç—Ä—É–¥", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "—É–≤–æ–ª—å–Ω", "–æ—Ç–ø—É—Å–∫", "–±–æ–ª—å–Ω–∏—á", "–¥–æ–≥–æ–≤–æ—Ä", "–º–∞p—à—Ä—É—Ç", "–æ–∫–ª–∞–¥"]

def detect_language_by_text(text: str):
    text = (text or "").strip()
    if not text:
        return None
    # –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ -> —Ä—É—Å—Å–∫–∏–π
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", text):
        return "ru"
    low = text.lower()
    # —è–≤–Ω—ã–µ —É–∑–±–µ–∫—Å–∫–∏–µ —Å–ª–æ–≤–∞ (latin)
    for k in UZ_KEYWORDS:
        if k in low:
            return "uz"
    # –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –±—É–∫–≤—ã –∏ –Ω–µ—Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ -> –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º uz
    if re.search(r"[A-Za-z]", text):
        if any(k in low for k in ["ish", "modda", "mehnat", "qonun", "salom"]):
            return "uz"
    return None

# —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —è–≤–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é
def extract_explicit_article(text: str):
    if not text:
        return None
    low = text.lower()
    m = re.search(r"—Å—Ç–∞—Ç(?:—å—è|–∏)?\s*(\d{1,4})", low)
    if m:
        return m.group(1)
    m = re.search(r"(\d{1,4})\s*-\s*modda", low) or re.search(r"(\d{1,4})\s+modda\b", low)
    if m:
        return m.group(1)
    m = re.search(r"modda\s*(\d{1,4})", low)
    if m:
        return m.group(1)
    return None

# -----------------------
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã–±–æ—Ä —è–∑—ã–∫–∞ –≤ –ø–∞–º—è—Ç–∏ (–≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Å—Å–∏–∏)
# -----------------------
USER_LANG = {}  # user_id -> "ru"/"uz"

# -----------------------
# /start
# -----------------------
@dp.message(CommandStart())
async def cmd_start(message: Message):
    await message.answer(
        "Salom! / –ü—Ä–∏–≤–µ—Ç! üëã\n\n"
        "–Ø ‚Äî HR-–ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –†–µ—Å–ø—É–±–ª–∏–∫–∏ –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω.\n"
        "–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ –ø–æ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–µ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞¬ª, ¬´modda 157¬ª –∏–ª–∏ ¬´mehnat shartnomasini bekor qilish¬ª).\n\n"
        "Agar tilni o‚Äòzgartirmoqchi bo‚Äòlsang ‚Äî yoz 'uz' yoki 'ru'.\n"
    )

# –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è —è–≤–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —è–∑—ã–∫–∞
@dp.message(F.text & F.regex(r"^(ru|uz)\b", flags=re.IGNORECASE))
async def set_lang_command(message: Message):
    code = message.text.strip().lower().split()[0]
    if code in ("ru", "uz"):
        USER_LANG[message.from_user.id] = code
        if code == "ru":
            await message.answer("–Ø–∑—ã–∫ –≤—ã–±—Ä–∞–Ω: –†—É—Å—Å–∫–∏–π üá∑üá∫")
        else:
            await message.answer("Til tanlandi: O ªzbekcha (latin) üá∫üáø")
    else:
        await message.answer("–ù–∞–ø–∏—à–∏—Ç–µ 'ru' –∏–ª–∏ 'uz' —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å —è–∑—ã–∫.")

# -----------------------
# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä HR / NOT_HR (GPT –∫–∞–∫ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
# -----------------------
async def classify_hr(question: str) -> str:
    # –∫–æ—Ä–æ—Ç–∫–æ: HR –∏–ª–∏ NOT_HR
    try:
        prompt = (
            "–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–æ: HR –∏–ª–∏ NOT_HR. HR ‚Äî –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É/—Ç—Ä—É–¥–æ–≤—ã–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º (—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ, –æ—Ç–ø—É—Å–∫, –æ–ø–ª–∞—Ç–∞, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ –∏ —Ç.–¥.). "
            "NOT_HR ‚Äî –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, —ç–º–æ—Ü–∏–∏, –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —Ç—Ä—É–¥–æ–≤–æ–º—É –ø—Ä–∞–≤—É —Å–æ–æ–±—â–µ–Ω–∏—è.\n\n"
            f"–¢–µ–∫—Å—Ç: {question}\n\n–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ 'HR' –∏–ª–∏ 'NOT_HR'."
        )
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role":"system","content":"–¢—ã ‚Äî –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –û—Ç–≤–µ—á–∞–π 'HR' –∏–ª–∏ 'NOT_HR'."},
                {"role":"user","content":prompt}
            ],
            temperature=0.0,
            max_tokens=6
        )
        out = resp.choices[0].message.content.strip().upper()
        if out.startswith("HR"):
            return "HR"
        return "NOT_HR"
    except Exception:
        low = (question or "").lower()
        if any(x in low for x in ["—É–≤–æ–ª", "–æ—Ç–ø—É—Å–∫", "–±–æ–ª", "–¥–æ–≥–æ–≤–æ—Ä", "—Ç—Ä—É–¥", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "modda", "ish", "mehnat"]):
            return "HR"
        return "NOT_HR"

# -----------------------
# –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
# -----------------------
@dp.message(F.text)
async def handle_message(message: Message):
    uid = message.from_user.id
    text = (message.text or "").strip()
    if not text:
        await message.answer("–ù–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –∏–ª–∏ 'ru'/'uz' –¥–ª—è –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞.")
        return

    # 1) —è–≤–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞
    if text.strip().lower() in ("ru", "uz"):
        USER_LANG[uid] = text.strip().lower()
        await message.answer("–Ø–∑—ã–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")
        return

    # 2) –ø–æ–ª—É—á–∞–µ–º —è–∑—ã–∫: —Å–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π, –ø–æ—Ç–æ–º –¥–µ—Ç–µ–∫—Ü–∏—è
    lang = USER_LANG.get(uid) or detect_language_by_text(text)

    # 3) –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ‚Äî —Å–ø—Ä–æ—Å–∏–º
    if not lang:
        await message.answer(
            "–ù–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫. –ù–∞–ø–∏—à–∏—Ç–µ 'ru' –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏–ª–∏ 'uz' –¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ (latin), –∑–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–æ–ø—Ä–æ—Å —Å–Ω–æ–≤–∞.\n\n"
            "Tilni aniqlay olmadim. Iltimos, 'ru' yoki 'uz' deb yozing."
        )
        return

    # —Å–æ—Ö—Ä–∞–Ω–∏–º –≤—ã–±–æ—Ä
    USER_LANG[uid] = lang

    # 4) –∫–æ—Ä–æ—Ç–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è HR/NOT_HR
    classification = "HR"
    try:
        classification = await classify_hr(text)
    except Exception:
        classification = "HR"

    if classification == "NOT_HR":
        if lang == "ru":
            await message.answer(
                "–ü—Ä–∏–≤–µ—Ç! –Ø HR-–±–æ—Ç –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É. –ó–∞–¥–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —É–≤–æ–ª—å–Ω–µ–Ω–∏–µ, –æ—Ç–ø—É—Å–∫, –±–æ–ª—å–Ω–∏—á–Ω—ã–π –∏–ª–∏ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä ‚Äî –ø–æ–º–æ–≥—É."
                f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {LEX_LINK}"
            )
        else:
            await message.answer(
                "Salom! Men Mehnat Kodeksi bo‚Äòyicha yordamchiman. Iltimos, aniq savol yozing ‚Äî men yordam beraman."
                f"\n\nManba: {LEX_LINK}"
            )
        return

    # 5) –ø–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —è–≤–Ω—É—é —Å—Ç–∞—Ç—å—é
    explicit = extract_explicit_article(text)
    found_article = None
    article_text = ""

    if explicit and explicit in LEX_BASE:
        found_article = explicit
        article_text = LEX_BASE.get(found_article, {}).get(lang) or LEX_BASE.get(found_article, {}).get("ru") or ""
    elif explicit:
        # explicit —É–∫–∞–∑–∞–Ω, –Ω–æ –Ω–µ—Ç –≤ –±–∞–∑–µ
        found_article = None
        article_text = ""

    # 6) semantic search –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–π —Å—Ç–∞—Ç—å–∏
    if not found_article and VECTORS:
        try:
            emb = client.embeddings.create(model="text-embedding-3-small", input=text)
            qvec = np.array(emb.data[0].embedding)
            best = None
            best_score = -1.0
            for aid, vec in VECTORS.items():
                sc = cosine(qvec, np.array(vec))
                if sc > best_score:
                    best_score = sc
                    best = aid
            # –ø–æ—Ä–æ–≥ ‚Äî —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π, –º–æ–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å
            if best is not None and best_score >= 0.23:
                found_article = str(best)
                article_text = LEX_BASE.get(found_article, {}).get(lang) or LEX_BASE.get(found_article, {}).get("ru") or ""
        except Exception:
            logging.exception("–û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞")

    # 7) —Å–æ—Å—Ç–∞–≤–ª—è—é prompt –¥–ª—è –æ—Ç–≤–µ—Ç–∞: –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –¥–∞—ë–º –µ—ë + –æ–±—ä—è—Å–Ω—è–µ–º; –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –¥–∞—ë–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç + —Å—Å—ã–ª–∫—É
    system_msg = (
        "–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π HR-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –∫–æ–¥–µ–∫—Å—É –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–∞. "
        "–û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –¥–∞–≤–∞–π –ø–æ–Ω—è—Ç–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è –∫–∞–¥—Ä–æ–≤–∏–∫–∞ –∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞. "
        "–ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –µ–≥–æ –∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤."
    )

    if found_article and article_text:
        user_msg = (
            f"–Ø–∑—ã–∫: {lang}\n–í–æ–ø—Ä–æ—Å: {text}\n\n–ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è #{found_article} (–ª–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞). –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:\n{article_text}\n\n"
            "–î–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–¥—Ä–æ–≤–∏–∫–∞."
        )
    else:
        user_msg = (
            f"–Ø–∑—ã–∫: {lang}\n–í–æ–ø—Ä–æ—Å: {text}\n\n–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏. "
            "–î–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π, –ø–æ–Ω—è—Ç–Ω—ã–π —Å–æ–≤–µ—Ç –∏ –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–¥–∞–∫—Ü–∏—é –Ω–∞ lex.uz."
        )

    # 8) –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.2,
            max_tokens=700
        )
        answer = resp.choices[0].message.content.strip()
    except Exception as e:
        logging.exception("GPT error: %s", e)
        if found_article and article_text:
            answer = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏. –ù–æ –≤–æ—Ç –Ω–∞–π–¥–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è:\n\n{article_text}"
        else:
            answer = "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    # 9) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ lex.uz
    footer = f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {LEX_LINK}"
    if found_article:
        header = f"–ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {found_article}\n\n"
        await message.answer(header + answer + footer)
    else:
        await message.answer(answer + footer)

# -----------------------
# –ó–∞–ø—É—Å–∫
# -----------------------
async def main():
    logging.basicConfig(level=logging.INFO)
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
